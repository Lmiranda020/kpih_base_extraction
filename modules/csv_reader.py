import pandas as pd
import chardet
import os

def detectar_tipo_arquivo(caminho_arquivo):
    """
    Detecta se o arquivo √© realmente CSV ou Excel (independente da extens√£o)
    
    Returns:
        'excel' ou 'csv'
    """
    try:
        with open(caminho_arquivo, 'rb') as f:
            # L√™ os primeiros bytes (assinatura do arquivo)
            primeiros_bytes = f.read(8)
            
            # Assinaturas de arquivos Excel
            # XLSX: 50 4B 03 04 (ZIP file, pois XLSX √© um ZIP)
            # XLS:  D0 CF 11 E0 (OLE2/CFB)
            if primeiros_bytes[:4] == b'PK\x03\x04':  # ZIP = XLSX
                return 'excel'
            elif primeiros_bytes[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':  # XLS
                return 'excel'
            else:
                return 'csv'
    except Exception:
        # Se der erro, assume CSV baseado na extens√£o
        return 'csv' if caminho_arquivo.lower().endswith('.csv') else 'excel'


def ler_csv_robusto(caminho_arquivo, sep=';', encoding='utf-8-sig'):
    """
    L√™ CSV ou Excel de forma robusta, detectando automaticamente o tipo
    
    Args:
        caminho_arquivo: Caminho do arquivo
        sep: Separador (padr√£o: ';')
        encoding: Encoding (padr√£o: 'utf-8-sig')
    
    Returns:
        DataFrame ou None em caso de erro
    """
    print(f"üìÇ Analisando: {caminho_arquivo}")
    
    # Detecta se √© realmente CSV ou Excel
    tipo_real = detectar_tipo_arquivo(caminho_arquivo)
    extensao = os.path.splitext(caminho_arquivo)[1].lower()
    
    print(f"   üìÑ Extens√£o: {extensao}")
    print(f"   üîç Tipo real: {tipo_real.upper()}")
    
    if extensao != f'.{tipo_real}':
        print(f"   ‚ö†Ô∏è ATEN√á√ÉO: Arquivo com extens√£o {extensao} mas conte√∫do √© {tipo_real.upper()}!")
    
    # Se for Excel (independente da extens√£o), l√™ como Excel
    if tipo_real == 'excel':
        print(f"   üìä Lendo como Excel...")
        try:
            df = pd.read_excel(caminho_arquivo, engine='openpyxl')
            print(f"   ‚úÖ Sucesso! {len(df)} linhas, {len(df.columns)} colunas")
            print(f"   üìã Colunas: {', '.join(df.columns[:5].tolist())}{'...' if len(df.columns) > 5 else ''}")
            return df
        except Exception as e:
            print(f"   ‚ùå Erro ao ler como Excel: {str(e)[:80]}")
            print(f"   üîÑ Tentando como CSV...")
    
    # Tenta como CSV
    tentativas = [
        # Tentativa 1: Como foi salvo (sep=';', encoding='utf-8-sig')
        {'sep': ';', 'encoding': 'utf-8-sig', 'engine': 'python'},
        
        # Tentativa 2: UTF-8 sem BOM
        {'sep': ';', 'encoding': 'utf-8', 'engine': 'python'},
        
        # Tentativa 3: Com on_bad_lines='skip' para pular linhas problem√°ticas
        {'sep': ';', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip'},
        
        # Tentativa 4: Com quoting para lidar com aspas
        {'sep': ';', 'encoding': 'utf-8-sig', 'engine': 'python', 
         'quoting': 1, 'on_bad_lines': 'skip'},  # QUOTE_MINIMAL
        
        # Tentativa 5: Detectar encoding automaticamente
        {'sep': ';', 'engine': 'python', 'on_bad_lines': 'skip'},
        
        # Tentativa 6: Tentar v√≠rgula como separador
        {'sep': ',', 'encoding': 'utf-8-sig', 'engine': 'python', 'on_bad_lines': 'skip'},
        
        # Tentativa 7: Com escapechar
        {'sep': ';', 'encoding': 'utf-8-sig', 'engine': 'python', 
         'escapechar': '\\', 'on_bad_lines': 'skip'},
        
        # Tentativa 8: Modo mais permissivo (avisa mas n√£o para)
        {'sep': ';', 'encoding': 'utf-8-sig', 'engine': 'python',
         'on_bad_lines': 'warn', 'quotechar': '"', 'doublequote': True}
    ]
    
    # Detecta encoding se necess√°rio
    encoding_detectado = None
    try:
        with open(caminho_arquivo, 'rb') as f:
            resultado = chardet.detect(f.read(100000))
            encoding_detectado = resultado['encoding']
            confianca = resultado['confidence']
            print(f"   üîç Encoding detectado: {encoding_detectado} (confian√ßa: {confianca:.0%})")
    except Exception:
        pass
    
    for i, kwargs in enumerate(tentativas, 1):
        # Usa encoding detectado na tentativa 5
        if i == 5 and encoding_detectado:
            kwargs['encoding'] = encoding_detectado
        
        try:
            print(f"   Tentativa {i}: sep='{kwargs.get('sep', ';')}' | encoding={kwargs.get('encoding', 'default')}")
            df = pd.read_csv(caminho_arquivo, **kwargs)
            
            if df.empty:
                print(f"   ‚ö†Ô∏è DataFrame vazio")
                continue
            
            if len(df.columns) < 2:
                print(f"   ‚ö†Ô∏è Apenas {len(df.columns)} coluna(s) - prov√°vel erro de separador")
                continue
            
            print(f"   ‚úÖ Sucesso! {len(df)} linhas, {len(df.columns)} colunas")
            print(f"   üìã Colunas: {', '.join(df.columns[:5].tolist())}{'...' if len(df.columns) > 5 else ''}")
            return df
            
        except Exception as e:
            print(f"   ‚ùå Falhou: {str(e)[:80]}")
            continue
    
    print(f"   ‚õî Todas as tentativas falharam")
    return None


def consolidar_dados_api_robusto(
    caminho_novo,
    caminho_anterior,
    colunas_chave,
    caminho_saida,
    nome_api="API"
):
    """
    Consolida dados de duas APIs de forma robusta
    
    Args:
        caminho_novo: Caminho do arquivo do m√™s atual
        caminho_anterior: Caminho do arquivo do m√™s anterior
        colunas_chave: Lista de colunas para identificar registros √∫nicos
        caminho_saida: Caminho onde salvar o consolidado
        nome_api: Nome da API (para logs)
    """
    print(f"\n{'='*60}")
    print(f"üîÑ Consolidando: {nome_api}")
    print(f"{'='*60}\n")
    
    # L√™ arquivo novo
    print("üì• Lendo arquivo do m√™s atual...")
    df_novo = ler_csv_robusto(caminho_novo, sep=';', encoding='utf-8-sig')
    
    if df_novo is None:
        print(f"‚ùå N√£o foi poss√≠vel ler o arquivo novo: {caminho_novo}")
        return None
    
    # L√™ arquivo anterior
    print("\nüì• Lendo arquivo do m√™s anterior...")
    df_antigo = ler_csv_robusto(caminho_anterior, sep=';', encoding='utf-8-sig')
    
    if df_antigo is None:
        print(f"‚ö†Ô∏è N√£o foi poss√≠vel ler o arquivo anterior: {caminho_anterior}")
        print(f"üíæ Salvando apenas dados do m√™s atual...")
        df_novo.to_csv(caminho_saida, index=False, sep=';', encoding='utf-8-sig')
        print(f"‚úÖ Arquivo salvo: {caminho_saida}")
        return caminho_saida
    
    # Valida colunas chave
    colunas_faltantes_novo = [col for col in colunas_chave if col not in df_novo.columns]
    colunas_faltantes_anterior = [col for col in colunas_chave if col not in df_antigo.columns]
    
    if colunas_faltantes_novo:
        print(f"‚ö†Ô∏è Colunas faltantes no arquivo novo: {colunas_faltantes_novo}")
    if colunas_faltantes_anterior:
        print(f"‚ö†Ô∏è Colunas faltantes no arquivo anterior: {colunas_faltantes_anterior}")
    
    # Remove duplicatas antes de concatenar
    print(f"\nüîÑ Removendo duplicatas...")
    df_novo_limpo = df_novo.drop_duplicates(subset=colunas_chave, keep='last')
    df_anterior_limpo = df_antigo.drop_duplicates(subset=colunas_chave, keep='last')
    
    duplicatas_novo = len(df_novo) - len(df_novo_limpo)
    duplicatas_anterior = len(df_antigo) - len(df_anterior_limpo)
    
    if duplicatas_novo > 0:
        print(f"   ‚ö†Ô∏è Removidas {duplicatas_novo} duplicata(s) do arquivo novo")
    if duplicatas_anterior > 0:
        print(f"   ‚ö†Ô∏è Removidas {duplicatas_anterior} duplicata(s) do arquivo anterior")
    
    # Concatena
    print(f"\nüîó Concatenando dados...")
    df_consolidado = pd.concat([df_anterior_limpo, df_novo_limpo], ignore_index=True)
    
    # Remove duplicatas finais (mant√©m o mais recente = do arquivo novo)
    df_final = df_consolidado.drop_duplicates(subset=colunas_chave, keep='last')
    
    # Estat√≠sticas
    total_registros = len(df_final)
    registros_novo = len(df_novo_limpo)
    registros_anterior = len(df_anterior_limpo)
    registros_unicos = total_registros
    
    print(f"\nüìä Resultado da consolida√ß√£o:")
    print(f"   Arquivo novo: {registros_novo} registros")
    print(f"   Arquivo anterior: {registros_anterior} registros")
    print(f"   Total ap√≥s consolida√ß√£o: {registros_unicos} registros √∫nicos")
    
    # Salva
    try:
        df_final.to_csv(caminho_saida, index=False, sep=';', encoding='utf-8-sig')
        print(f"\n‚úÖ Arquivo consolidado salvo:")
        print(f"   üìç {caminho_saida}")
        print(f"{'='*60}\n")
        return caminho_saida
    except Exception as e:
        print(f"\n‚ùå Erro ao salvar arquivo consolidado: {e}")
        return None
