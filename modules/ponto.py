"""
M√≥dulo para an√°lise incremental de compet√™ncias com consolida√ß√£o robusta
Vers√£o corrigida - Dezembro 2024
"""
import os
import pandas as pd
import shutil
import glob
from datetime import datetime
from dateutil.relativedelta import relativedelta
from dotenv import load_dotenv


class AnalisadorIncremental:
    """Gerencia a an√°lise incremental de compet√™ncias entre meses"""
    
    def __init__(self, caminho_atual):
        """
        Inicializa o analisador
        
        Args:
            caminho_atual: Caminho do diret√≥rio do m√™s vigente
        """
        self.caminho_atual = caminho_atual
        self.caminho_mes_1 = None  # M√™s -1
        self.caminho_mes_2 = None  # M√™s -2
        self._obter_caminhos_meses_anteriores()
        
    def _obter_caminhos_meses_anteriores(self):
        """
        Identifica os diret√≥rios dos 2 meses anteriores
        """
        load_dotenv()
        caminho_fixo = os.getenv("caminho_fixo")
        
        if not caminho_fixo:
            print("‚ùå Vari√°vel 'caminho_fixo' n√£o encontrada no .env")
            return
        
        # Extrai informa√ß√µes do caminho atual
        partes = self.caminho_atual.split(os.sep)
        pasta_competencia = partes[-1]  # Ex: "11_2024"
        
        try:
            mes_atual, ano_atual = pasta_competencia.split('_')
            mes_atual = int(mes_atual)
            ano_atual = int(ano_atual)
        except ValueError:
            print(f"‚ùå Formato inv√°lido da pasta: {pasta_competencia}")
            return
        
        # Calcula m√™s -1
        if mes_atual == 1:
            mes_1 = 12
            ano_1 = ano_atual - 1
        else:
            mes_1 = mes_atual - 1
            ano_1 = ano_atual
        
        # Calcula m√™s -2
        if mes_1 == 1:
            mes_2 = 12
            ano_2 = ano_1 - 1
        else:
            mes_2 = mes_1 - 1
            ano_2 = ano_1
        
        # Monta caminhos
        pasta_mes_1 = f"{mes_1:02d}_{ano_1}"
        pasta_mes_2 = f"{mes_2:02d}_{ano_2}"
        
        self.caminho_mes_1 = os.path.join(caminho_fixo, str(ano_1), pasta_mes_1)
        self.caminho_mes_2 = os.path.join(caminho_fixo, str(ano_2), pasta_mes_2)
        
        # Verifica exist√™ncia
        print("\nüìÇ VERIFICANDO MESES ANTERIORES:")
        
        if os.path.exists(self.caminho_mes_1):
            print(f"‚úÖ M√™s -1 encontrado: {pasta_mes_1}")
        else:
            print(f"‚ö†Ô∏è M√™s -1 N√ÉO encontrado: {pasta_mes_1}")
            self.caminho_mes_1 = None
        
        if os.path.exists(self.caminho_mes_2):
            print(f"‚úÖ M√™s -2 encontrado: {pasta_mes_2}")
        else:
            print(f"‚ö†Ô∏è M√™s -2 N√ÉO encontrado: {pasta_mes_2}")
            self.caminho_mes_2 = None
    
    def _carregar_competencias_mes(self, caminho_mes, rotulo):
        """
        Carrega arquivo de compet√™ncias de um m√™s espec√≠fico
        
        Args:
            caminho_mes: Caminho do diret√≥rio do m√™s
            rotulo: R√≥tulo para log (ex: "M√™s -1")
            
        Returns:
            DataFrame ou None
        """
        if not caminho_mes:
            return None
        
        arquivo = os.path.join(caminho_mes, "competencias_todas_unidades.xlsx")
        
        if not os.path.exists(arquivo):
            print(f"‚ö†Ô∏è {rotulo}: Arquivo n√£o encontrado")
            return None
        
        try:
            df = pd.read_excel(arquivo)
            print(f"‚úÖ {rotulo}: {len(df)} compet√™ncias carregadas")
            return df
        except Exception as e:
            print(f"‚ùå {rotulo}: Erro ao carregar - {e}")
            return None
    
    def filtrar_competencias_nao_processadas(self, arquivo_competencia_atual, 
                                             processar_somente_fechadas=True):
        """
        Filtra compet√™ncias usando an√°lise de 2 meses anteriores
        
        Args:
            arquivo_competencia_atual: Caminho do arquivo de compet√™ncias do m√™s vigente
            processar_somente_fechadas: Se True, processa apenas compet√™ncias fechadas
            
        Returns:
            str: Caminho do arquivo filtrado ou None
        """
        print("\n" + "="*70)
        print("üîç AN√ÅLISE INCREMENTAL COM 2 MESES ANTERIORES")
        print("="*70)
        
        # Carregar compet√™ncias do m√™s atual
        if not os.path.exists(arquivo_competencia_atual):
            print(f"‚ùå Arquivo n√£o encontrado: {arquivo_competencia_atual}")
            return None
        
        df_atual = pd.read_excel(arquivo_competencia_atual)
        total_inicial = len(df_atual)
        print(f"\nüìä M√äS ATUAL: {total_inicial} compet√™ncias")
        
        # Carregar compet√™ncias dos meses anteriores
        print("\nüìÇ Carregando hist√≥rico...")
        df_mes_1 = self._carregar_competencias_mes(self.caminho_mes_1, "M√™s -1")
        df_mes_2 = self._carregar_competencias_mes(self.caminho_mes_2, "M√™s -2")
        
        # Filtrar compet√™ncias do m√™s atual
        if processar_somente_fechadas:
            df_atual_filtrado = df_atual[
                (df_atual['situacao'] != 'ABERTA') & 
                (df_atual['situacao'] != 'REABERTA')
            ].copy()
            
            print(f"\nüîí Filtro aplicado: apenas compet√™ncias FECHADAS")
            print(f"   ‚Ä¢ Total no m√™s atual: {total_inicial}")
            print(f"   ‚Ä¢ Fechadas: {len(df_atual_filtrado)}")
        else:
            df_atual_filtrado = df_atual.copy()
            print(f"\nüîì Sem filtro de status - processando TODAS as compet√™ncias")
        
        if df_atual_filtrado.empty:
            print("\n‚ö†Ô∏è Nenhuma compet√™ncia para processar no m√™s atual!")
            return None
        
        # Se n√£o h√° hist√≥rico, processa tudo
        if df_mes_1 is None and df_mes_2 is None:
            print("\n‚ö†Ô∏è SEM HIST√ìRICO - Primeira execu√ß√£o")
            print("   ‚û°Ô∏è Processando TODAS as compet√™ncias dispon√≠veis")
            
            nome_filtrado = "competencias_todas_unidades_filtrado.xlsx"
            caminho_filtrado = os.path.join(
                os.path.dirname(arquivo_competencia_atual), 
                nome_filtrado
            )
            df_atual_filtrado.to_excel(caminho_filtrado, index=False)
            print(f"\nüíæ Arquivo salvo: {caminho_filtrado}")
            return caminho_filtrado
        
        # Criar chave √∫nica para compara√ß√£o
        print("\nüîë Criando chaves de identifica√ß√£o...")
        
        df_atual_filtrado['chave'] = (
            df_atual_filtrado['nome'] + '_' + df_atual_filtrado['competencia']
        )
        
        if df_mes_1 is not None:
            df_mes_1['chave'] = df_mes_1['nome'] + '_' + df_mes_1['competencia']
        
        if df_mes_2 is not None:
            df_mes_2['chave'] = df_mes_2['nome'] + '_' + df_mes_2['competencia']
        
        # Identificar status nos meses anteriores
        print("\nüîç AN√ÅLISE DE STATUS:")
        
        fechadas_mes_1 = set()
        reabertas_mes_1 = set()
        if df_mes_1 is not None:
            fechadas_mes_1 = set(
                df_mes_1[
                    (df_mes_1['situacao'] != 'ABERTA') & 
                    (df_mes_1['situacao'] != 'REABERTA')
                ]['chave']
            )
            reabertas_mes_1 = set(
                df_mes_1[df_mes_1['situacao'] == 'REABERTA']['chave']
            )
            print(f"   ‚Ä¢ M√™s -1: {len(fechadas_mes_1)} fechadas | {len(reabertas_mes_1)} reabertas")
        
        fechadas_mes_2 = set()
        reabertas_mes_2 = set()
        if df_mes_2 is not None:
            fechadas_mes_2 = set(
                df_mes_2[
                    (df_mes_2['situacao'] != 'ABERTA') & 
                    (df_mes_2['situacao'] != 'REABERTA')
                ]['chave']
            )
            reabertas_mes_2 = set(
                df_mes_2[df_mes_2['situacao'] == 'REABERTA']['chave']
            )
            print(f"   ‚Ä¢ M√™s -2: {len(fechadas_mes_2)} fechadas | {len(reabertas_mes_2)} reabertas")
        
        # Regra de exclus√£o
        print("\nüßÆ APLICANDO REGRA DE EXCLUS√ÉO:")
        
        excluir = set()
        
        if df_mes_1 is not None and df_mes_2 is not None:
            excluir = fechadas_mes_1 & fechadas_mes_2
            print(f"   ‚Ä¢ Fechadas em AMBOS os meses: {len(excluir)}")
        elif df_mes_1 is not None:
            excluir = fechadas_mes_1
            print(f"   ‚Ä¢ Fechadas no m√™s -1: {len(excluir)}")
        elif df_mes_2 is not None:
            excluir = fechadas_mes_2
            print(f"   ‚Ä¢ Fechadas no m√™s -2: {len(excluir)}")
        
        # Detectar compet√™ncias REABERTAS
        competencias_para_reprocessar = set()
        
        if df_mes_1 is not None and df_mes_2 is not None:
            chaves_atuais_fechadas = set(df_atual_filtrado['chave'])
            
            cenario_1 = fechadas_mes_2 & reabertas_mes_1 & chaves_atuais_fechadas
            cenario_2 = reabertas_mes_2 & fechadas_mes_1 & chaves_atuais_fechadas
            
            competencias_para_reprocessar = cenario_1 | cenario_2
            
            print(f"\nüîÑ COMPET√äNCIAS DETECTADAS PARA REPROCESSAMENTO:")
            print(f"   ‚Ä¢ Cen√°rio 1 (Fechada ‚Üí Reaberta ‚Üí Fechada): {len(cenario_1)}")
            print(f"   ‚Ä¢ Cen√°rio 2 (Reaberta ‚Üí Fechada ‚Üí Fechada): {len(cenario_2)}")
            print(f"   ‚Ä¢ TOTAL: {len(competencias_para_reprocessar)}")
            
            excluir = excluir - competencias_para_reprocessar
        
        # Aplicar filtro
        print(f"\nüìä RESUMO:")
        print(f"   ‚Ä¢ Total no m√™s atual (filtrado): {len(df_atual_filtrado)}")
        print(f"   ‚Ä¢ A excluir (j√° processadas): {len(excluir)}")
        
        df_final = df_atual_filtrado[
            ~df_atual_filtrado['chave'].isin(excluir)
        ].copy()
        
        df_final = df_final.drop(columns=['chave'])
        
        total_final = len(df_final)
        removidos = len(df_atual_filtrado) - total_final
        
        print(f"   ‚Ä¢ Removidas: {removidos}")
        print(f"   ‚Ä¢ ‚úÖ RESTANTES PARA PROCESSAR: {total_final}")
        
        if df_final.empty:
            print("\n‚ö†Ô∏è NENHUMA COMPET√äNCIA NOVA PARA PROCESSAR!")
            return None
        
        nome_filtrado = "competencias_todas_unidades_filtrado.xlsx"
        caminho_filtrado = os.path.join(
            os.path.dirname(arquivo_competencia_atual), 
            nome_filtrado
        )
        
        df_final.to_excel(caminho_filtrado, index=False)
        print(f"\n‚úÖ Arquivo filtrado salvo: {caminho_filtrado}")
        
        return caminho_filtrado
    
    def _buscar_arquivo_api(self, diretorio, nome_base):
        """
        Busca arquivo de uma API (CSV ou XLSX) de forma robusta
        
        Args:
            diretorio: Diret√≥rio onde buscar
            nome_base: Nome base do arquivo (ex: 'api_estatistica')
            
        Returns:
            tuple: (caminho_completo, extensao) ou (None, None)
        """
        # Tenta encontrar arquivo com qualquer extens√£o
        for extensao in ['.csv', '.xlsx']:
            # Padr√£o: api_estatistica*.csv ou api_estatistica*.xlsx
            padrao = os.path.join(diretorio, f"{nome_base}*{extensao}")
            arquivos = glob.glob(padrao)
            
            if arquivos:
                # Retorna o primeiro encontrado
                return arquivos[0], extensao
        
        return None, None
    
    def _carregar_arquivo_api(self, caminho_arquivo, extensao):
        """
        Carrega arquivo CSV ou XLSX de forma robusta
        
        Args:
            caminho_arquivo: Caminho completo do arquivo
            extensao: '.csv' ou '.xlsx'
            
        Returns:
            DataFrame ou None
        """
        try:
            if extensao == '.csv':
                # Importa fun√ß√£o robusta de leitura CSV
                from modules.csv_reader import ler_csv_robusto
                return ler_csv_robusto(caminho_arquivo, sep=';', encoding='utf-8-sig')
            else:  # .xlsx
                return pd.read_excel(caminho_arquivo)
        except Exception as e:
            print(f"   ‚ùå Erro ao carregar arquivo: {e}")
            return None
    
    def consolidar_dados_api(self, nome_arquivo_api):
        """
        Consolida dados de uma API: novos (m√™s atual) + hist√≥rico (m√™s -1)
        VERS√ÉO CORRIGIDA - Busca robusta e ordem correta
        
        Args:
            nome_arquivo_api: Nome base do arquivo (ex: 'api_estatistica.csv')
            
        Returns:
            bool: True se consolidou com sucesso
        """
        # Remove extens√£o para busca
        nome_base = nome_arquivo_api.replace('.csv', '').replace('.xlsx', '')
        
        print(f"\nüîÑ Consolidando: {nome_base}")
        
        # ================================================================
        # PASSO 1: Buscar arquivo NOVO (m√™s atual)
        # ================================================================
        arquivo_novo, extensao_novo = self._buscar_arquivo_api(self.caminho_atual, nome_base)
        
        if not arquivo_novo:
            print(f"   ‚ö†Ô∏è Arquivo n√£o encontrado no m√™s atual")
            return False
        
        print(f"   üìÑ Novo: {os.path.basename(arquivo_novo)}")
        
        # ================================================================
        # PASSO 2: Carregar arquivo NOVO
        # ================================================================
        df_novo = self._carregar_arquivo_api(arquivo_novo, extensao_novo)
        
        if df_novo is None:
            print(f"   ‚ùå Falha ao carregar arquivo novo")
            return False
        
        print(f"   üìä Registros novos: {len(df_novo):,}")
        
        # ================================================================
        # PASSO 3: Verificar se h√° m√™s -1
        # ================================================================
        if not self.caminho_mes_1:
            print(f"   ‚ÑπÔ∏è Sem m√™s -1 - mantendo apenas dados novos")
            return True
        
        # ================================================================
        # PASSO 4: Buscar arquivo ANTIGO (m√™s -1)
        # ================================================================
        arquivo_antigo, extensao_antigo = self._buscar_arquivo_api(self.caminho_mes_1, nome_base)
        
        if not arquivo_antigo:
            print(f"   ‚ÑπÔ∏è Arquivo n√£o encontrado no m√™s -1 - mantendo apenas dados novos")
            return True
        
        print(f"   üìÑ Antigo: {os.path.basename(arquivo_antigo)}")
        
        # ================================================================
        # PASSO 5: Carregar arquivo ANTIGO
        # ================================================================
        df_antigo = self._carregar_arquivo_api(arquivo_antigo, extensao_antigo)
        
        if df_antigo is None:
            print(f"   ‚ö†Ô∏è Falha ao carregar arquivo antigo - mantendo apenas dados novos")
            return True
        
        print(f"   üìä Registros antigos: {len(df_antigo):,}")
        
        # ================================================================
        # PASSO 6: CONSOLIDAR - ORDEM CORRETA!
        # ================================================================
        try:
            # IMPORTANTE: Novo primeiro, antigo depois
            # Ao remover duplicatas com keep='first', mant√©m os NOVOS
            df_consolidado = pd.concat([df_novo, df_antigo], ignore_index=True)
            
            tamanho_antes = len(df_consolidado)
            df_consolidado = df_consolidado.drop_duplicates(keep='first')
            duplicatas = tamanho_antes - len(df_consolidado)
            
            if duplicatas > 0:
                print(f"   üóëÔ∏è Duplicatas removidas: {duplicatas}")
             
            print(f"   ‚úÖ Total consolidado: {len(df_consolidado):,}")
            
            # ================================================================
            # PASSO 7: Salvar no formato ORIGINAL
            # ================================================================
            if extensao_novo == '.csv':
                df_consolidado.to_csv(arquivo_novo, index=False, sep=';', encoding='utf-8-sig')
            else:
                df_consolidado.to_excel(arquivo_novo, index=False)
            
            print(f"   üíæ Arquivo consolidado salvo: {os.path.basename(arquivo_novo)}")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Erro ao consolidar: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def consolidar_todas_apis(self, nomes_arquivos_apis):
        """
        Consolida dados de m√∫ltiplas APIs
        """
        print("\n" + "="*60)
        print("üì¶ CONSOLIDANDO DADOS DAS APIs")
        print("="*60)
        
        resultados = {}
        
        for nome_arquivo in nomes_arquivos_apis:
            sucesso = self.consolidar_dados_api(nome_arquivo)
            resultados[nome_arquivo] = sucesso
        
        print("\n" + "="*60)
        print("üìã RESUMO DA CONSOLIDA√á√ÉO")
        print("="*60)
        
        sucessos = sum(1 for v in resultados.values() if v)
        total = len(resultados)
        
        for arquivo, sucesso in resultados.items():
            status = "‚úÖ" if sucesso else "‚ùå"
            nome_base = arquivo.replace('.csv', '').replace('.xlsx', '')
            print(f"{status} {nome_base}")
        
        print(f"\nüìä Total: {sucessos}/{total} consolida√ß√µes bem-sucedidas")
        
        return resultados
    
    def consolidar_dados_api_inteligente(self, nome_arquivo_api):
        """
        Consolida dados mantendo APENAS os dados MAIS RECENTES
        Remove duplicatas mantendo dados NOVOS, descarta ANTIGOS
        """
        nome_base = nome_arquivo_api.replace('.csv', '').replace('.xlsx', '')
        
        print(f"\nüîÑ Consolidando: {nome_base}")
        
        # Buscar arquivo NOVO (m√™s atual)
        arquivo_novo, extensao_novo = self._buscar_arquivo_api(self.caminho_atual, nome_base)
        
        if not arquivo_novo:
            print(f"   ‚ö†Ô∏è Arquivo n√£o encontrado no m√™s atual")
            return False
        
        print(f"   üìÑ Novo: {os.path.basename(arquivo_novo)}")
        
        # Carregar arquivo NOVO
        df_novo = self._carregar_arquivo_api(arquivo_novo, extensao_novo)
        
        if df_novo is None:
            print(f"   ‚ùå Falha ao carregar arquivo novo")
            return False
        
        registros_novos = len(df_novo)
        print(f"   üìä Registros novos: {registros_novos:,}")
        
        # Verificar se h√° m√™s -1
        if not self.caminho_mes_1:
            print(f"   ‚ÑπÔ∏è Sem m√™s -1 - mantendo apenas dados novos")
            return True
        
        # Buscar arquivo ANTIGO (m√™s -1)
        arquivo_antigo, extensao_antigo = self._buscar_arquivo_api(self.caminho_mes_1, nome_base)
        
        if not arquivo_antigo:
            print(f"   ‚ÑπÔ∏è Arquivo n√£o encontrado no m√™s -1 - mantendo apenas dados novos")
            return True
        
        print(f"   üìÑ Antigo: {os.path.basename(arquivo_antigo)}")
        
        # Carregar arquivo ANTIGO
        df_antigo = self._carregar_arquivo_api(arquivo_antigo, extensao_antigo)
        
        if df_antigo is None:
            print(f"   ‚ö†Ô∏è Falha ao carregar arquivo antigo - mantendo apenas dados novos")
            return True
        
        registros_antigos = len(df_antigo)
        print(f"   üìä Registros antigos: {registros_antigos:,}")
        
        # Identificar colunas-chave
        colunas_chave = self._identificar_colunas_chave(df_novo, nome_base)
        
        if not colunas_chave:
            print(f"   ‚ö†Ô∏è N√£o foi poss√≠vel identificar colunas-chave")
            df_consolidado = pd.concat([df_antigo, df_novo], ignore_index=True)
        else:
            print(f"   üîë Colunas-chave: {', '.join(colunas_chave[:3])}{'...' if len(colunas_chave) > 3 else ''}")
            
            # CR√çTICO: NOVO primeiro, ANTIGO depois
            # drop_duplicates(keep='first') mant√©m dados NOVOS
            df_consolidado = pd.concat([df_novo, df_antigo], ignore_index=True)
            
            tamanho_antes = len(df_consolidado)
            df_consolidado = df_consolidado.drop_duplicates(subset=colunas_chave, keep='first')
            
            duplicatas_removidas = tamanho_antes - len(df_consolidado)
            
            if duplicatas_removidas > 0:
                print(f"   üóëÔ∏è Duplicatas removidas: {duplicatas_removidas:,}")
                print(f"      (Mantidos dados NOVOS, removidos ANTIGOS)")
        
        registros_finais = len(df_consolidado)
        print(f"   ‚úÖ Total consolidado: {registros_finais:,}")
        
        # Salvar
        try:
            if extensao_novo == '.csv':
                df_consolidado.to_csv(arquivo_novo, index=False, sep=';', encoding='utf-8-sig')
            else:
                df_consolidado.to_excel(arquivo_novo, index=False)
            
            print(f"   üíæ Arquivo consolidado salvo")
            return True
            
        except Exception as e:
            print(f"   ‚ùå Erro ao salvar: {e}")
            return False

    def _identificar_colunas_chave(self, df, nome_api):
        """Identifica colunas-chave para remo√ß√£o de duplicatas"""
        colunas = df.columns.tolist()
        colunas_lower = [c.lower() for c in colunas]
        
        # Mapeamento espec√≠fico por API
        chaves_especificas = {
            'custosindividualizadoporcentro': ['centroDeCustoDescr', 'competenciaDescr', 'contaDescr', 'grupoContaDescr', 'tipoDescr', 'classificacaoDescr', 'unidade' ],
            'folhadepagamento': ['contaDeCustoDescr', 'centroDeCustoDescr', 'competenciaDescr', 'nomeFuncionario', 'unidade'],
            'notasfiscais': ['contaDeCustoDescr', 'centroDeCustoDescr', 'competenciaDescr', 'numero', 'fornecedor', 'unidade'],
            'quantidadecirurgia': ['centroDeCustoDescr', 'competenciaDescr', 'unidade'],
            'quantidadeleito': ['centroDeCustoDescr', 'competenciaDescr', 'unidade'],
            'consumo': ['contaDeCustoDescr', 'centroDeCustoDescr', 'competenciaDescr', 'itemDeEstoque', 'codigoTUSS' ,'unidade'],
            'benchmarkcomposicaodecustos': ['tipoCentroCusto', 'unidade', 'competencia'],
            'demonstracaocustounitariodosservicosauxiliares': ['competenciaDescr', 'grupo', 'descricao', 'unidade'],
            'custounitarioporponderacao': ['competenciaDescr', 'centroDeCustoDescr', 'criterioDeRateioDescr', 'ponderacaoDeRateioDescr', 'unidade'],
            'composicaoevolucaodereceita': ['tipo', 'grupoDaContaDescr', 'contaDescr', 'competenciaDescr', 'unidade'],
            'analisedepartamental': ['grupoContaDeCustoDescr', 'centroDeCustoDescr', 'competenciaDescr', 'unidade'],
            'custoporespecialidade': ['especialidadeDescr', 'centroCustoDestinoDescr', 'centroCustoOrigenDescr', 'unidadeProducaoDescr', 'competenciaDescr', 'unidade'],
            'painelcomparativodecustos': ['unidadeDeProducaoId', 'unidadeDeProducaoDescr', 'competencia'],
            'evolucaodecustos': ['grupoDaContaDescr', 'contaDeCustoDescr', 'competenciaDescr', 'tipoContaDeCustoDescr', 'classificacaoDoCustoDescr', 'unidade'], 
            'rankingdecusto': ['grupoDoCentroDescr', 'centroDeCustoDescr', 'competenciaDescr', 'unidade'],
            'estatistica': ['grupoDoCentroDescr','centroDeCustoDescr', 'competenciaDescr', 'criterioDeRateioDescr', 'unidade'],
            'composicaodecustos': ['grupoDaContaDescr', 'contaDeCustoDescr', 'tipoContaDeCustoDescr', 'competenciaDescr', 'tipo_composicao', 'unidade'],
            'demonstracaocustounitarioporsaida': ['especialidadeDescr', 'competenciaDescr', 'unidade'],
            'demonstracaocustounitario': ['centroDeCustoDescr', 'competenciaDescr', 'unidade'],
            'producoes': ['centroDeCustoDescr', 'competenciaDescr', 'unidadeDeProducaoDescr', 'unidade'],
        }
        
        # Tenta identificar a API
        for api_key, chaves in chaves_especificas.items():
            if api_key in nome_api.lower():
                chaves_encontradas = []
                for chave in chaves:
                    if chave.lower() in colunas_lower:
                        idx = colunas_lower.index(chave.lower())
                        chaves_encontradas.append(colunas[idx])
                
                if chaves_encontradas:
                    return chaves_encontradas
        
        # Fallback: busca 'competencia' + 'nome'
        chaves_encontradas = []
        for chave_comum in ['unidade', 'competencia', 'unidade']:
            if chave_comum.lower() in colunas_lower:
                idx = colunas_lower.index(chave_comum.lower())
                chaves_encontradas.append(colunas[idx])
        
        return chaves_encontradas if len(chaves_encontradas) >= 2 else []

    def consolidar_todas_apis_inteligente(self, nomes_arquivos_apis):
        """Consolida m√∫ltiplas APIs mantendo dados MAIS RECENTES"""
        print("\n" + "="*60)
        print("üì¶ CONSOLIDANDO DADOS DAS APIs")
        print("üß† Modo: INTELIGENTE (mant√©m novos, remove antigos)")
        print("="*60)
        
        resultados = {}
        
        for nome_arquivo in nomes_arquivos_apis:
            sucesso = self.consolidar_dados_api_inteligente(nome_arquivo)
            resultados[nome_arquivo] = sucesso
        
        print("\n" + "="*60)
        print("üìã RESUMO DA CONSOLIDA√á√ÉO")
        print("="*60)
        
        sucessos = sum(1 for v in resultados.values() if v)
        total = len(resultados)
        
        for arquivo, sucesso in resultados.items():
            status = "‚úÖ" if sucesso else "‚ùå"
            nome_base = arquivo.replace('.csv', '').replace('.xlsx', '')
            print(f"{status} {nome_base}")
        
        print(f"\nüìä Total: {sucessos}/{total} consolida√ß√µes")
        
        return resultados
    
    def copiar_arquivos_mes_anterior(self, nomes_arquivos_apis):
        """
        Copia arquivos das APIs do m√™s -1 para o m√™s atual
        """
        print("\n" + "="*60)
        print("üìÇ COPIANDO ARQUIVOS DO M√äS ANTERIOR")
        print("="*60)
        
        if not self.caminho_mes_1:
            print("‚ùå N√£o h√° m√™s -1 dispon√≠vel")
            return {}
        
        resultados = {}
        
        for nome_arquivo_base in nomes_arquivos_apis:
            nome_base = nome_arquivo_base.replace('.csv', '').replace('.xlsx', '')
            
            # Busca arquivo no m√™s -1
            arquivo_origem, extensao = self._buscar_arquivo_api(self.caminho_mes_1, nome_base)
            
            if not arquivo_origem:
                print(f"‚ö†Ô∏è {nome_base} - n√£o encontrado no m√™s -1")
                resultados[nome_arquivo_base] = False
                continue
            
            # Destino: mant√©m o nome original
            nome_arquivo_real = os.path.basename(arquivo_origem)
            arquivo_destino = os.path.join(self.caminho_atual, nome_arquivo_real)
            
            try:
                shutil.copy2(arquivo_origem, arquivo_destino)
                print(f"‚úÖ {nome_arquivo_real} - copiado com sucesso")
                resultados[nome_arquivo_base] = True
            except Exception as e:
                print(f"‚ùå {nome_arquivo_real} - erro ao copiar: {e}")
                resultados[nome_arquivo_base] = False
        
        sucessos = sum(1 for v in resultados.values() if v)
        total = len(resultados)
        
        print(f"\nüìä Resumo: {sucessos}/{total} arquivos copiados com sucesso")
        
        return resultados


def processar_incremental(caminho_atual, arquivo_competencia_atual, nomes_arquivos_apis,
                         processar_somente_fechadas=True):
    """
    Fun√ß√£o principal para processamento incremental
    """
    print("\n" + "="*60)
    print("üöÄ INICIANDO PROCESSAMENTO INCREMENTAL")
    print("="*60)
    
    analisador = AnalisadorIncremental(caminho_atual)
    
    arquivo_filtrado = analisador.filtrar_competencias_nao_processadas(
        arquivo_competencia_atual,
        processar_somente_fechadas=processar_somente_fechadas
    )
    
    if arquivo_filtrado is None:
        print("\n" + "="*60)
        print("üìã MODO: C√ìPIA (sem novas compet√™ncias)")
        print("="*60)
        
        resultados_copia = analisador.copiar_arquivos_mes_anterior(nomes_arquivos_apis)
        
        print("\n" + "="*60)
        print("‚úÖ PROCESSAMENTO INCREMENTAL CONCLU√çDO (MODO C√ìPIA)")
        print("="*60 + "\n")
        
        return None, resultados_copia, 'copiar'
    
    print("\n" + "="*60)
    print("üìã MODO: PROCESSAMENTO (h√° novas compet√™ncias)")
    print("="*60)
    print("\n‚ö†Ô∏è Consolida√ß√£o ser√° executada ap√≥s extra√ß√£o dos dados novos")
    
    return arquivo_filtrado, {}, 'processar'


def consolidar_apos_extracao(caminho_atual, nomes_arquivos_apis):
    """
    Consolida dados das APIs ap√≥s a extra√ß√£o
    """
    print("\n" + "="*60)
    print("üì¶ CONSOLIDANDO DADOS (NOVOS + M√äS ANTERIOR)")
    print("="*60)   
    
    analisador = AnalisadorIncremental(caminho_atual)
    resultados = analisador.consolidar_todas_apis_inteligente(nomes_arquivos_apis)
    
    print("\n" + "="*60)
    print("‚úÖ CONSOLIDA√á√ÉO CONCLU√çDA")
    print("="*60 + "\n")
    
    return resultados